{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 2\n",
    "\n",
    "In this homework, you will be implementing a neural network library in the needle framework. Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
      "  Cloning https://github.com/dlsys10714/mugrade.git to /private/var/folders/mt/8yf4wtss6nz03lqjqs60zg600000gn/T/pip-req-build-n0bv2qea\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /private/var/folders/mt/8yf4wtss6nz03lqjqs60zg600000gn/T/pip-req-build-n0bv2qea\n",
      "  Resolved https://github.com/dlsys10714/mugrade.git to commit 98609ee80ee24bf278455b48aa8d06bd3f5d0430\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: mugrade\n",
      "  Building wheel for mugrade (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mugrade: filename=mugrade-1.2-py3-none-any.whl size=3943 sha256=be668ac40e9e290a935d4bf73a9e169fa3166a13eb7fc838e6384dcbaf961a45\n",
      "  Stored in directory: /private/var/folders/mt/8yf4wtss6nz03lqjqs60zg600000gn/T/pip-ephem-wheel-cache-ww1vrhiu/wheels/a0/50/01/9ecbdf2bdad5e2f6cc3fe8099a7b961e953b2e5c111aa158eb\n",
      "Successfully built mugrade\n",
      "Installing collected packages: mugrade\n",
      "Successfully installed mugrade-1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Code to set up the assignment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "# !mkdir -p 10714\n",
    "# %cd /content/drive/MyDrive/10714\n",
    "# !git clone https://github.com/dlsys10714/hw2.git\n",
    "# %cd /content/drive/MyDrive/10714/hw2\n",
    "\n",
    "!pip install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0\n",
    "\n",
    "This homework builds off of Homework 1. First, in your Homework 2 directory, go to the files `autograd.py`, `ops.py` in the `python/needle` directory, and fill in the code between `### BEGIN YOUR SOLUTION` and `### END YOUR SOLUTION` with your solutions from Homework 1. \n",
    "\n",
    "**Note:** We have added functionality since HW1, specifically in the `autograd.py` file, so it is important to copy over your solutions rather than the entire previous file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1\n",
    "\n",
    "In this first question, you will implement a few different methods for weight initialization.  This will be done in the `python/needle/init.py` file, which contains a number of routines for initializing needle Tensors using various random and constant initializations.  Following the same methodology of the existing initializers (you will want to call e.g. `init.rand` or `init.randn` from your functions below, implement the following common initialization methods.  In all cases, the functions should return `fan_in` by `fan_out` 2D tensors (extensions to other sizes can be done via e.g., reshaping).\n",
    "\n",
    "\n",
    "### Xavier uniform\n",
    "`xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{U}(-a, a)$ where \n",
    "\\begin{equation}\n",
    "a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "Pass remaining `**kwargs` parameters to the corresponding `init` random call.\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `gain` - optional scaling factor\n",
    "___\n",
    "\n",
    "### Xavier normal\n",
    "`xavier_normal(fan_in, fan_out, gain=1.0, **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a normal distribution. The resulting Tensor will have values sampled from $\\mathcal{N}(0, \\text{std}^2)$ where \n",
    "\\begin{equation}\n",
    "\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `gain` - optional scaling factor\n",
    "___\n",
    "\n",
    "### Kaiming uniform\n",
    "`kaiming_uniform(fan_in, fan_out, nonlinearity=\"relu\", **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{U}(-\\text{bound}, \\text{bound})$ where \n",
    "\\begin{equation}\n",
    "\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan_in}}}\n",
    "\\end{equation}\n",
    "\n",
    "Use the recommended gain value for ReLU: $\\text{gain}=\\sqrt{2}$.\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `nonlinearity` - the non-linear function\n",
    "___\n",
    "\n",
    "### Kaiming normal\n",
    "`kaiming_normal(fan_in, fan_out, nonlinearity=\"relu\", **kwargs)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{N}(0, \\text{std}^2)$ where \n",
    "\\begin{equation}\n",
    "\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan_in}}}\n",
    "\\end{equation}\n",
    "\n",
    "Use the recommended gain value for ReLU: $\\text{gain}=\\sqrt{2}$.\n",
    "\n",
    "##### Parameters\n",
    "- `fan_in` - dimensionality of input\n",
    "- `fan_out` - dimensionality of output\n",
    "- `nonlinearity` - the non-linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 86 deselected / 4 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_init_kaiming_uniform \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_init_kaiming_normal \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_init_xavier_uniform \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_init_xavier_normal \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m86 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_init\" -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting init...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 2.73s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"init\" -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "In this question, you will implement additional modules in `python/needle/nn.py`. Specifically, for the following modules described below, initialize any variables of the module in the constructor, and fill out the `forward` method. \n",
    "___\n",
    "\n",
    "### Linear\n",
    "`needle.nn.Linear(in_features, out_features, bias=True, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$. The input shape is $(N, H_{in})$ where $H_{in}=\\text{in_features}$. The output shape is $(N, H_{out})$ where $H_{out}=\\text{out_features}$.\n",
    "\n",
    "Be careful to explicitly broadcast the bias term to the correct shape -- Needle does not support implicit broadcasting.\n",
    "\n",
    "Additionally note that, for all layers including this one, you should initialize the weight Tensor before the bias Tensor, and should initialize all Parameters using only functions from 'init'.\n",
    "\n",
    "##### Parameters\n",
    "- `in_features` - size of each input sample\n",
    "- `out_features` - size of each output sample\n",
    "- `bias` - If set to `False`, the layer will not learn an additive bias.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of shape (`in_features`, `out_features`). The values should be initialized with the Kaiming Uniform initialization with `fan_in = in_features`\n",
    "- `bias` - the learnable bias of shape (`out_features`). The values should be initialized with the Kaiming Uniform initialize with `fan_in = out_features`. **Note the different in fan_in choice, due to their relative sizes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 91 items / 83 deselected / 8 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_linear_weight_init_1 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_bias_init_1 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_forward_1 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_forward_2 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_forward_3 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_backward_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_backward_2 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_linear_backward_3 [[[ 16.318823    0.3890714  -2.3196607 -10.607947   -8.891977\n",
      "    16.04581     9.475689   14.571134    6.581477   10.204643 ]\n",
      "  [ 20.291656    7.48733     1.2581345 -14.285493   -6.0252004\n",
      "    19.621624    4.343303    6.973201   -0.8103489   4.037069 ]\n",
      "  [ 11.332953   -5.698288   -8.815561   -7.673438   -7.6161675\n",
      "     9.361553   17.341637   17.269142   18.1076     14.261493 ]]]\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_nn_linear_backward_1 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_linear_backward_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(linear_backward((\u001b[94m10\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m)),\n",
      "            np.array([[ \u001b[94m20.61148\u001b[39;49;00m,   \u001b[94m6.920893\u001b[39;49;00m,  -\u001b[94m1.625556\u001b[39;49;00m, -\u001b[94m13.497676\u001b[39;49;00m,  -\u001b[94m6.672813\u001b[39;49;00m,\n",
      "                   \u001b[94m18.762121\u001b[39;49;00m,   \u001b[94m7.286628\u001b[39;49;00m,   \u001b[94m8.18535\u001b[39;49;00m ,   \u001b[94m2.741301\u001b[39;49;00m,   \u001b[94m5.723689\u001b[39;49;00m]],\n",
      "             dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:572: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:130: in linear_backward\n",
      "    (f(x)**\u001b[94m2\u001b[39;49;00m).sum().backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in gradient\n",
      "    \u001b[94mreturn\u001b[39;49;00m (out_grad.sum(\u001b[96mtuple\u001b[39;49;00m(axes)).reshape(node_input_shape), )\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:399: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.Reshape(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:222: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in reshape\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m:298: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mreshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newshape, order=order)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = 53.95241, method = 'reshape', args = ((5,),), kwds = {'order': 'C'}\n",
      "bound = <built-in method reshape of numpy.float32 object at 0x12093ad30>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapfunc\u001b[39;49;00m(obj, method, *args, **kwds):\n",
      "        bound = \u001b[96mgetattr\u001b[39;49;00m(obj, method, \u001b[94mNone\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m bound \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\n",
      "    \n",
      "        \u001b[94mtry\u001b[39;49;00m:\n",
      ">           \u001b[94mreturn\u001b[39;49;00m bound(*args, **kwds)\n",
      "\u001b[1m\u001b[31mE           ValueError: cannot reshape array of size 1 into shape (5,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m:57: ValueError\n",
      "\u001b[31m\u001b[1m__________________________ test_nn_linear_backward_2 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_nn_linear_backward_2\u001b[39;49;00m():\n",
      ">       \u001b[96mprint\u001b[39;49;00m(linear_backward((\u001b[94m10\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m3\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m)))\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:578: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:130: in linear_backward\n",
      "    (f(x)**\u001b[94m2\u001b[39;49;00m).sum().backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in gradient\n",
      "    \u001b[94mreturn\u001b[39;49;00m (out_grad.sum(\u001b[96mtuple\u001b[39;49;00m(axes)).reshape(node_input_shape), )\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:399: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.Reshape(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:222: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in reshape\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m:298: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mreshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newshape, order=order)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = 168.47995, method = 'reshape', args = ((5,),), kwds = {'order': 'C'}\n",
      "bound = <built-in method reshape of numpy.float32 object at 0x120cdacd0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapfunc\u001b[39;49;00m(obj, method, *args, **kwds):\n",
      "        bound = \u001b[96mgetattr\u001b[39;49;00m(obj, method, \u001b[94mNone\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m bound \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\n",
      "    \n",
      "        \u001b[94mtry\u001b[39;49;00m:\n",
      ">           \u001b[94mreturn\u001b[39;49;00m bound(*args, **kwds)\n",
      "\u001b[1m\u001b[31mE           ValueError: cannot reshape array of size 1 into shape (5,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m:57: ValueError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_nn_linear_backward_1\u001b[0m - ValueError: cannot reshape array of size 1 into shape (5,)\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_nn_linear_backward_2\u001b[0m - ValueError: cannot reshape array of size 1 into shape (5,)\n",
      "\u001b[31m================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m6 passed\u001b[0m, \u001b[33m83 deselected\u001b[0m\u001b[31m in 0.33s\u001b[0m\u001b[31m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_linear\" -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_linear...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "\u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ submit_nn_linear _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92msubmit_nn_linear\u001b[39;49;00m():\n",
      "        mugrade.submit(linear_forward((\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)))\n",
      "        mugrade.submit(linear_forward((\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)))\n",
      "        mugrade.submit(linear_forward((\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)))\n",
      ">       mugrade.submit(linear_backward((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)))\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:601: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:130: in linear_backward\n",
      "    (f(x)**\u001b[94m2\u001b[39;49;00m).sum().backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:247: in gradient\n",
      "    \u001b[94mreturn\u001b[39;49;00m (out_grad.sum(\u001b[96mtuple\u001b[39;49;00m(axes)).reshape(node_input_shape), )\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:399: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.Reshape(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:222: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.reshape(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in reshape\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m:298: in reshape\n",
      "    \u001b[94mreturn\u001b[39;49;00m _wrapfunc(a, \u001b[33m'\u001b[39;49;00m\u001b[33mreshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newshape, order=order)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "obj = 43.021744, method = 'reshape', args = ((5,),), kwds = {'order': 'C'}\n",
      "bound = <built-in method reshape of numpy.float32 object at 0x115a3cc30>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_wrapfunc\u001b[39;49;00m(obj, method, *args, **kwds):\n",
      "        bound = \u001b[96mgetattr\u001b[39;49;00m(obj, method, \u001b[94mNone\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m bound \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m _wrapit(obj, method, *args, **kwds)\n",
      "    \n",
      "        \u001b[94mtry\u001b[39;49;00m:\n",
      ">           \u001b[94mreturn\u001b[39;49;00m bound(*args, **kwds)\n",
      "\u001b[1m\u001b[31mE           ValueError: cannot reshape array of size 1 into shape (5,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m:57: ValueError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1msubmit_nn_linear\u001b[0m - ValueError: cannot reshape array of size 1 into shape (5,)\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[31m in 3.51s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "`needle.nn.ReLU()`\n",
    "\n",
    "Applies the rectified linear unit function element-wise:\n",
    "$ReLU(x) = max(0, x)$.\n",
    "\n",
    "If you have previously implemented ReLU's backwards pass in terms of itself, note that this is numerically unstable and will likely cause problems\n",
    "down the line.\n",
    "Instead, consider that we could write the derivative of ReLU as $I\\{x>0\\}$, where we arbitrarily decide that the derivative at $x=0$ is 0.\n",
    "(This is a _subdifferentiable_ function.)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 88 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_relu_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m                [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_relu_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m88 deselected\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_relu...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 1.67s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_relu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sequential\n",
    "`needle.nn.Sequential(*modules)`\n",
    "\n",
    "Applies a sequence of modules to the input (in the order that they were passed to the constructor) and returns the output of the last module.\n",
    "These should be kept in a `.module` property: you should _not_ redefine any magic methods like `__getitem__`, as this may not be compatible with our tests.\n",
    "\n",
    "##### Parameters\n",
    "- `*modules` - any number of modules of type `needle.nn.Module`\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 88 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_sequential_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_sequential_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m88 deselected\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_sequential...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 1.80s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_sequential\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LogSumExp\n",
    "\n",
    "`needle.ops.LogSumExp(axes)`\n",
    "\n",
    "Here you will need to implement one additional operatior in the `python/ops.py` file, as you did in HW1. Applies a numerically stable log-sum-exp function to the input by subtracting off the maximum elements.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{LogSumExp}(z) = \\log (\\sum_{i} \\exp (z_i - \\max{z})) + \\max{z}\n",
    "\\end{equation}\n",
    "\n",
    "#### Parameters\n",
    "- `axes` - Tuple of axes to sum and take the maximum element over. This uses the same conventions as `needle.ops.Summation()`\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 80 deselected / 10 selected                               \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_forward_1 \u001b[31mFAILED\u001b[0m\u001b[31m           [ 10%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_forward_2 \u001b[32mPASSED\u001b[0m\u001b[31m           [ 20%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_forward_3 \u001b[31mFAILED\u001b[0m\u001b[31m           [ 30%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_forward_4 \u001b[31mFAILED\u001b[0m\u001b[31m           [ 40%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_forward_5 \u001b[32mPASSED\u001b[0m\u001b[31m           [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_backward_1 \u001b[31mFAILED\u001b[0m\u001b[31m          [ 60%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_backward_2 \u001b[31mFAILED\u001b[0m\u001b[31m          [ 70%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_backward_3 \u001b[31mFAILED\u001b[0m\u001b[31m          [ 80%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_backward_5 \u001b[31mFAILED\u001b[0m\u001b[31m          [ 90%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_op_logsumexp_backward_4 \u001b[31mFAILED\u001b[0m\u001b[31m          [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_forward_1 __________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_forward_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_forward((\u001b[94m3\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m),(\u001b[94m1\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m)),\n",
      "            np.array([\u001b[94m5.366029\u001b[39;49;00m , \u001b[94m4.9753823\u001b[39;49;00m, \u001b[94m6.208126\u001b[39;49;00m ], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Mismatched elements: 3 / 3 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max absolute difference: 0.51233673\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max relative difference: 0.0825268\u001b[0m\n",
      "\u001b[1m\u001b[31mE        x: array([4.97835 , 4.796401, 6.720463], dtype=float32)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        y: array([5.366029, 4.975382, 6.208126], dtype=float32)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:391: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_forward_3 __________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_forward_3\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_forward((\u001b[94m1\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m,\u001b[94m4\u001b[39;49;00m),(\u001b[94m0\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m)),\n",
      "            np.array([[\u001b[94m5.276974\u001b[39;49;00m , \u001b[94m5.047317\u001b[39;49;00m , \u001b[94m3.778802\u001b[39;49;00m , \u001b[94m5.0103745\u001b[39;49;00m],\n",
      "           [\u001b[94m5.087831\u001b[39;49;00m , \u001b[94m4.391712\u001b[39;49;00m , \u001b[94m5.025037\u001b[39;49;00m , \u001b[94m2.0214698\u001b[39;49;00m]], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:399: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:286: in logsumexp_forward\n",
      "    \u001b[94mreturn\u001b[39;49;00m (ndl.ops.logsumexp(x,axes=axes)).cached_data\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:406: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x107aa4e20>\n",
      "Z = array([[[[4.2 , 4.5 , 1.9 , 4.85],\n",
      "         [4.85, 3.3 , 2.7 , 3.05],\n",
      "         [0.3 , 3.65, 3.1 , 0.1 ]],\n",
      "\n",
      "        [[4.5 , 4.05, 3.05, 0.15],\n",
      "         [3.  , 1.65, 4.85, 1.3 ],\n",
      "         [3.95, 2.9 , 1.2 , 1.  ]]]], dtype=float32)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        max_z = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.log(array_api.exp(Z - max_z).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)) + max_z\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with shapes (1,2,3,4) (2,4)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:396: ValueError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_forward_4 __________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_forward_4\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_forward((\u001b[94m3\u001b[39;49;00m,\u001b[94m10\u001b[39;49;00m),(\u001b[94m1\u001b[39;49;00m,)),\n",
      "            np.array([\u001b[94m5.705309\u001b[39;49;00m, \u001b[94m5.976375\u001b[39;49;00m, \u001b[94m5.696459\u001b[39;49;00m], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:405: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:286: in logsumexp_forward\n",
      "    \u001b[94mreturn\u001b[39;49;00m (ndl.ops.logsumexp(x,axes=axes)).cached_data\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:406: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x107b08c10>\n",
      "Z = array([[3.85, 3.25, 3.5 , 0.5 , 0.75, 4.95, 3.6 , 0.85, 1.7 , 1.65],\n",
      "       [4.35, 2.  , 2.2 , 4.9 , 2.  , 2.7 , 3.35, 3.  , 1.6 , 4.5 ],\n",
      "       [3.35, 2.35, 1.8 , 4.8 , 2.15, 3.  , 4.3 , 2.25, 0.75, 2.85]],\n",
      "      dtype=float32)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, Z):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        max_z = array_api.max(Z, axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      ">       \u001b[94mreturn\u001b[39;49;00m array_api.log(array_api.exp(Z - max_z).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)) + max_z\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with shapes (3,10) (3,)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:396: ValueError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_1 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_backward((\u001b[94m3\u001b[39;49;00m,\u001b[94m1\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m,)),\n",
      "            np.array([[\u001b[94m1.\u001b[39;49;00m ],\n",
      "           [\u001b[94m7.3\u001b[39;49;00m],\n",
      "           [\u001b[94m9.9\u001b[39;49;00m]], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:414: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:291: in logsumexp_backward\n",
      "    y.backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x1172d9a90>\n",
      "out_grad = needle.Tensor([ 1.1061918 13.706192  18.90619  ])\n",
      "node = needle.Tensor([0.5530959 6.853096  9.453095 ])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:401: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_2 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_2\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_backward((\u001b[94m3\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m)),\n",
      "            np.array([[[\u001b[94m1.4293308\u001b[39;49;00m , \u001b[94m1.2933122\u001b[39;49;00m , \u001b[94m0.82465225\u001b[39;49;00m],\n",
      "            [\u001b[94m0.50017685\u001b[39;49;00m, \u001b[94m2.1323113\u001b[39;49;00m , \u001b[94m2.1323113\u001b[39;49;00m ],\n",
      "            [\u001b[94m1.4293308\u001b[39;49;00m , \u001b[94m0.58112264\u001b[39;49;00m, \u001b[94m0.40951014\u001b[39;49;00m]],\n",
      "    \n",
      "           [[\u001b[94m0.3578173\u001b[39;49;00m , \u001b[94m0.07983983\u001b[39;49;00m, \u001b[94m4.359107\u001b[39;49;00m  ],\n",
      "            [\u001b[94m1.1300558\u001b[39;49;00m , \u001b[94m0.561169\u001b[39;49;00m  , \u001b[94m0.1132981\u001b[39;49;00m ],\n",
      "            [\u001b[94m0.9252113\u001b[39;49;00m , \u001b[94m0.65198547\u001b[39;49;00m, \u001b[94m1.7722803\u001b[39;49;00m ]],\n",
      "    \n",
      "           [[\u001b[94m0.2755132\u001b[39;49;00m , \u001b[94m2.365242\u001b[39;49;00m  , \u001b[94m2.888913\u001b[39;49;00m  ],\n",
      "            [\u001b[94m0.05291228\u001b[39;49;00m, \u001b[94m1.1745441\u001b[39;49;00m , \u001b[94m0.02627547\u001b[39;49;00m],\n",
      "            [\u001b[94m2.748018\u001b[39;49;00m  , \u001b[94m0.13681579\u001b[39;49;00m, \u001b[94m2.748018\u001b[39;49;00m  ]]], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:420: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:291: in logsumexp_backward\n",
      "    y.backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x107b51910>\n",
      "out_grad = needle.Tensor([ 9.956699  9.592802 13.440926])\n",
      "node = needle.Tensor([4.9783497 4.796401  6.720463 ])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:401: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_3 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_3\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_backward((\u001b[94m3\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m)),\n",
      "            np.array([[[\u001b[94m0.92824626\u001b[39;49;00m, \u001b[94m0.839912\u001b[39;49;00m  , \u001b[94m0.5355515\u001b[39;49;00m ],\n",
      "            [\u001b[94m0.59857905\u001b[39;49;00m, \u001b[94m2.551811\u001b[39;49;00m  , \u001b[94m2.551811\u001b[39;49;00m  ],\n",
      "            [\u001b[94m1.0213376\u001b[39;49;00m , \u001b[94m0.41524494\u001b[39;49;00m, \u001b[94m0.29261813\u001b[39;49;00m]],\n",
      "    \n",
      "           [[\u001b[94m0.16957533\u001b[39;49;00m, \u001b[94m0.03783737\u001b[39;49;00m, \u001b[94m2.0658503\u001b[39;49;00m ],\n",
      "            [\u001b[94m0.98689\u001b[39;49;00m   , \u001b[94m0.49007502\u001b[39;49;00m, \u001b[94m0.09894446\u001b[39;49;00m],\n",
      "            [\u001b[94m0.48244575\u001b[39;49;00m, \u001b[94m0.3399738\u001b[39;49;00m , \u001b[94m0.9241446\u001b[39;49;00m ]],\n",
      "    \n",
      "           [[\u001b[94m0.358991\u001b[39;49;00m  , \u001b[94m3.081887\u001b[39;49;00m  , \u001b[94m3.764224\u001b[39;49;00m  ],\n",
      "            [\u001b[94m0.12704718\u001b[39;49;00m, \u001b[94m2.820187\u001b[39;49;00m  , \u001b[94m0.06308978\u001b[39;49;00m],\n",
      "            [\u001b[94m3.9397335\u001b[39;49;00m , \u001b[94m0.19614778\u001b[39;49;00m, \u001b[94m3.9397335\u001b[39;49;00m ]]], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:435: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:291: in logsumexp_backward\n",
      "    y.backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x107aa59d0>\n",
      "out_grad = needle.Tensor([12.61717   9.713573 11.71887 ])\n",
      "node = needle.Tensor([6.308585  4.8567867 5.859435 ])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:401: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_5 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_5\u001b[39;49;00m():\n",
      "        grad_compare = ndl.Tensor(np.array([[\u001b[94m1e10\u001b[39;49;00m,\u001b[94m1e9\u001b[39;49;00m,\u001b[94m1e8\u001b[39;49;00m,-\u001b[94m10\u001b[39;49;00m],[\u001b[94m1e-10\u001b[39;49;00m,\u001b[94m1e9\u001b[39;49;00m,\u001b[94m1e8\u001b[39;49;00m,-\u001b[94m10\u001b[39;49;00m]]))\n",
      ">       test_data = (ndl.ops.logsumexp(grad_compare, (\u001b[94m0\u001b[39;49;00m,))**\u001b[94m2\u001b[39;49;00m).sum().backward()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:450: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x107acea90>\n",
      "out_grad = needle.Tensor([ 2.00000000e+10  2.00000000e+09  2.00000001e+08 -1.86137056e+01])\n",
      "node = needle.Tensor([ 1.00000000e+10  1.00000000e+09  1.00000001e+08 -9.30685282e+00])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:401: NotImplementedError\n",
      "\u001b[31m\u001b[1m_________________________ test_op_logsumexp_backward_4 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_logsumexp_backward_4\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(logsumexp_backward((\u001b[94m1\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m,\u001b[94m4\u001b[39;49;00m), \u001b[94mNone\u001b[39;49;00m),\n",
      "            np.array([[[[\u001b[94m0.96463485\u001b[39;49;00m, \u001b[94m1.30212122\u001b[39;49;00m, \u001b[94m0.09671321\u001b[39;49;00m, \u001b[94m1.84779774\u001b[39;49;00m],\n",
      "             [\u001b[94m1.84779774\u001b[39;49;00m, \u001b[94m0.39219132\u001b[39;49;00m, \u001b[94m0.21523925\u001b[39;49;00m, \u001b[94m0.30543892\u001b[39;49;00m],\n",
      "             [\u001b[94m0.01952606\u001b[39;49;00m, \u001b[94m0.55654611\u001b[39;49;00m, \u001b[94m0.32109909\u001b[39;49;00m, \u001b[94m0.01598658\u001b[39;49;00m]],\n",
      "    \n",
      "            [[\u001b[94m1.30212122\u001b[39;49;00m, \u001b[94m0.83026929\u001b[39;49;00m, \u001b[94m0.30543892\u001b[39;49;00m, \u001b[94m0.01680623\u001b[39;49;00m],\n",
      "             [\u001b[94m0.29054249\u001b[39;49;00m, \u001b[94m0.07532032\u001b[39;49;00m, \u001b[94m1.84779774\u001b[39;49;00m, \u001b[94m0.05307731\u001b[39;49;00m],\n",
      "             [\u001b[94m0.75125862\u001b[39;49;00m, \u001b[94m0.26289377\u001b[39;49;00m, \u001b[94m0.04802637\u001b[39;49;00m, \u001b[94m0.03932065\u001b[39;49;00m]]]], dtype=np.float32), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:472: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:291: in logsumexp_backward\n",
      "    y.backward()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:340: in backward\n",
      "    compute_gradient_of_variables(\u001b[96mself\u001b[39;49;00m, out_grad)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:434: in compute_gradient_of_variables\n",
      "    input_grads = node.op.gradient_as_tuple(node_grad, node)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:113: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.LogSumExp object at 0x107b1f6d0>\n",
      "out_grad = needle.Tensor(13.707964897155762), node = needle.Tensor(6.8539824)\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m()\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:401: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_forward_1\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_forward_3\u001b[0m - ValueError: operands could not be broadcast together with shapes (1,2,3,4) ...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_forward_4\u001b[0m - ValueError: operands could not be broadcast together with shapes (3,10) (3,)\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_1\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_2\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_3\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_5\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_op_logsumexp_backward_4\u001b[0m - NotImplementedError\n",
      "\u001b[31m================== \u001b[31m\u001b[1m8 failed\u001b[0m, \u001b[32m2 passed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 0.42s\u001b[0m\u001b[31m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_op_logsumexp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = 4, 2\n",
    "\n",
    "def f(x1, x2):\n",
    "    return x1 * x1 + 3 * x2\n",
    "\n",
    "def der_f_x1(x1, x2):\n",
    "    return 2 * x1\n",
    "\n",
    "def der_f_x2(x1, x2):\n",
    "    return 3\n",
    "\n",
    "def max_f(x1, x2):\n",
    "    return x1\n",
    "\n",
    "def der_max_f_x1(x1, x2):\n",
    "    return 1\n",
    "\n",
    "def der_max_f_x2(x1, x2):\n",
    "    return 0\n",
    "\n",
    "grad = [der_max_f_x1(x1, x2), der_max_f_x2(x1, x2)]\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum t1: tensor([0.7330, 0.3056, 0.6157, 0.6260, 0.9024, 0.8419, 0.9345, 0.8124, 0.9472,\n",
      "        0.7988], requires_grad=True)\n",
      "max: tensor(0.9472, grad_fn=<MaxBackward1>) requires_grad: True\n",
      "t1 gradients: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(10, requires_grad=True)\n",
    "t2 = torch.rand(10, requires_grad=True)\n",
    "\n",
    "\n",
    "s1 = torch.sum(t1)\n",
    "s2 = torch.sum(t2)\n",
    "# print('sum t1:', s1, 'sum t2:', s2)\n",
    "print('sum t1:', t1)\n",
    "m = torch.max(t1)\n",
    "print('max:', m, 'requires_grad:', m.requires_grad)\n",
    "m.backward()\n",
    "print('t1 gradients:', t1.grad)\n",
    "# print('t2 gradients:', t2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting op_logsumexp...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 6.52s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"op_logsumexp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SoftmaxLoss\n",
    "\n",
    "`needle.nn.SoftmaxLoss()`\n",
    "\n",
    "Applies the softmax loss as defined below (and as implemented in Homework 1), taking in as input a Tensor of logits and a Tensor of the true labels (expressed as a list of numbers, *not* one-hot encoded).\n",
    "\n",
    "Note that you can use the `init.one_hot` function now instead of writing this yourself.  Note: You will need to use the numerically stable logsumexp operated you just implemented for this purpose.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_\\text{softmax}(z,y) = \\log \\sum_{i=1}^k \\exp z_i - z_y\n",
    "\\end{equation}\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 91 items / 87 deselected / 4 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_softmax_loss_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 25%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_softmax_loss_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_softmax_loss_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 75%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_softmax_loss_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m87 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_softmax_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_softmax_loss...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 3.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_softmax_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LayerNorm1d\n",
    "`needle.nn.LayerNorm1d(dim, eps=1e-5, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies layer normalization over a mini-batch of inputs as described in the paper [Layer Normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "\\begin{equation}\n",
    "y = w \\circ \\frac{x_i - \\textbf{E}[x]}{((\\textbf{Var}[x]+\\epsilon)^{1/2})} + b\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textbf{E}[x]$ denotes the empirical mean of the inputs, $\\textbf{Var}[x]$ denotes their empirical variance (not that here we are using the \"unbiased\" estimate of the variance, i.e., dividing by $N$ rather than by $N-1$), and $w$ and $b$ denote learnable scalar weights and biases respectively.  Note you can assume the input to this layer by be a 2D tensor, with batches in the first dimension and features on the second.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` - number of channels\n",
    "- `eps` - a value added to the denominator for numerical stability.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of size `dim`, elements initialized to 1.\n",
    "- `bias` - the learnable bias of shape `dim`, elements initialized to 0 **(changed from 1)**.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29687455, 0.41836894, 0.07671091, 0.7216107 ],\n",
       "       [0.31205179, 0.92096634, 0.57094028, 0.87037498],\n",
       "       [0.40470171, 0.8935522 , 0.55037021, 0.27961685],\n",
       "       [0.75212383, 0.47058167, 0.99692825, 0.04713359],\n",
       "       [0.45549308, 0.0687375 , 0.88550218, 0.77414924]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(5, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37839127, 0.66858335, 0.53206024, 0.56669183, 0.5459705 ])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = a.sum(axis=1) / 4\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08151673,  0.03997766, -0.30168036,  0.34321942],\n",
       "       [-0.35653156,  0.25238299, -0.09764307,  0.20179163],\n",
       "       [-0.12735853,  0.36149196,  0.01830997, -0.25244339],\n",
       "       [ 0.18543199, -0.09611016,  0.43023641, -0.51955825],\n",
       "       [-0.09047742, -0.47723299,  0.33953168,  0.22817874]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_arg = a - np.expand_dims(E, axis=0).T\n",
    "sigma_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00664498, 0.00159821, 0.09101104, 0.11779957],\n",
       "       [0.12711475, 0.06369717, 0.00953417, 0.04071986],\n",
       "       [0.0162202 , 0.13067644, 0.00033525, 0.06372766],\n",
       "       [0.03438502, 0.00923716, 0.18510337, 0.26994077],\n",
       "       [0.00818616, 0.22775133, 0.11528176, 0.05206554]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = np.power((sigma_arg), 2)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05426345, 0.06026649, 0.05273989, 0.12466658, 0.1008212 ])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = sigma.sum(axis=1) / 4\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34990731,  0.17160253, -1.29495095,  1.47325574],\n",
       "       [-1.45219187,  1.02798342, -0.39771086,  0.8219193 ],\n",
       "       [-0.55452009,  1.5739389 ,  0.07972174, -1.09914055],\n",
       "       [ 0.52516071, -0.27219294,  1.21846966, -1.47143742],\n",
       "       [-0.284933  , -1.50291008,  1.06925881,  0.71858428]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sigma_arg) / np.power(sigma + 1e-5, 1 / 2).reshape((5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08151673,  0.03997766, -0.30168036,  0.34321942],\n",
       "       [-0.35653156,  0.25238299, -0.09764307,  0.20179163],\n",
       "       [-0.12735853,  0.36149196,  0.01830997, -0.25244339],\n",
       "       [ 0.18543199, -0.09611016,  0.43023641, -0.51955825],\n",
       "       [-0.09047742, -0.47723299,  0.33953168,  0.22817874]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 91 items / 84 deselected / 7 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [ 14%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m           [ 28%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m           [ 42%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 57%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 71%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_backward_3 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 85%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_layernorm_backward_4 \u001b[32mPASSED\u001b[0m\u001b[32m          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m7 passed\u001b[0m, \u001b[33m84 deselected\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_layernorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_layernorm...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 6.32s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_layernorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Flatten\n",
    "`needle.nn.Flatten()`\n",
    "\n",
    "Takes in a tensor of shape `(B,X_0,X_1,...)`, and flattens all non-batch dimensions so that the output is of shape `(B, X_0 * X_1 * ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 91 items / 82 deselected / 9 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m             [ 11%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m             [ 22%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m             [ 33%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_forward_4 \u001b[32mPASSED\u001b[0m\u001b[32m             [ 44%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 55%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 66%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_backward_3 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 77%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_backward_4 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 88%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_flatten_backward_5 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m9 passed\u001b[0m, \u001b[33m82 deselected\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_flatten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_flatten...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 6.02s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_flatten\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm1d\n",
    "`needle.nn.BatchNorm1d(dim, eps=1e-5, momentum=0.1, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies batch normalization over a mini-batch of inputs as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "\\begin{equation}\n",
    "y = w \\circ \\frac{z_i - \\textbf{E}[x]}{((\\textbf{Var}[x]+\\epsilon)^{1/2})} + b\n",
    "\\end{equation}\n",
    "\n",
    "but where here the mean and variance refer to to the mean and variance over the _batch_dimensions.  The function also computes a running average of mean/variance for all features at each layer $\\hat{\\mu}, \\hat{\\sigma}^2$, and at test time normalizes by these quantities:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\frac{(x - \\hat{mu})}{((\\hat{\\sigma}^2_{i+1})_j+\\epsilon)^{1/2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "BatchNorm uses the running estimates of mean and variance instead of batch statistics at test time, i.e.,\n",
    "after `model.eval()` has been called on the BatchNorm layer's `training` flag is false.\n",
    "\n",
    "To compute the running estimates, you can use the equation $$\\hat{x_{new}} = (1 - m) \\hat{x_{old}} + mx_{observed},$$\n",
    "where $m$ is momentum.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` - input dimension\n",
    "- `eps` - a value added to the denominator for numerical stability.\n",
    "- `momentum` - the value used for the running mean and running variance computation.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of size `dim`, elements initialized to 1.\n",
    "- `bias` - the learnable bias of size `dim`, elements initialized to 0.\n",
    "- `running_mean` - the running mean used at evaluation time, elements initialized to 0.\n",
    "- `running_var` - the running (unbiased) variance used at evaluation time, elements initialized to 1. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 82 deselected / 8 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_check_model_eval_switches_training_flag_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [ 25%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_forward_affine_1 \u001b[32mPASSED\u001b[0m\u001b[32m    [ 37%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_backward_affine_1 \u001b[32mPASSED\u001b[0m\u001b[32m   [ 62%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_running_mean_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 75%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_running_var_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 87%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_batchnorm_running_grad_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m8 passed\u001b[0m, \u001b[33m82 deselected\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_batchnorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_batchnorm...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 5.24s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_batchnorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "`needle.nn.Dropout(p = 0.5)`\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper [Improving neural networks by preventing co-adaption of feature detectors](https://arxiv.org/abs/1207.0580). During evaluation the module simply computes an identity function. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{z}_{i+1} = \\sigma_i (W_i^T z_i + b_i) \\\\\n",
    "(z_{i+1})_j = \n",
    "    \\begin{cases}\n",
    "    (\\hat{z}_{i+1})_j /(1-p) & \\text{with probability } 1-p \\\\\n",
    "    0 & \\text{with probability } p \\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "**Important**: If the Dropout module the flag `training=False`, you shouldn't \"dropout\" any weights. That is, dropout applies during training only, not during evaluation. Note that `training` is a flag in `nn.Module`.\n",
    "\n",
    "##### Parameters\n",
    "- `p` - the probability of an element to be zeroed.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 91 items / 89 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_dropout_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m             [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_dropout_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m89 deselected\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_dropout...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 2.55s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Residual\n",
    "`needle.nn.Residual(fn: Module)`\n",
    "\n",
    "Applies a residual or skip connection given module $\\mathcal{F}$ and input Tensor $x$, returning $\\mathcal{F}(x) + x$.\n",
    "##### Parameters\n",
    "- `fn` - module of type `needle.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 91 items / 89 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_nn_residual_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_nn_residual_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m89 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_residual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting nn_residual...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 2.69s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"nn_residual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Implement the `step` function of the following optimizers.\n",
    "Make sure that your optimizers _don't_ modify the gradients of tensors in-place.\n",
    "\n",
    "We have included some tests to ensure that you are not consuming excessive memory, which can happen if you are\n",
    "not using `.data` or `.detach()` in the right places, thus building an increasingly large computational graph\n",
    "(not just in the optimizers, but in the previous modules as well).\n",
    "You can ignore these tests, which include the string `memory_check` at your own discretion.\n",
    "\n",
    "___\n",
    "\n",
    "### SGD\n",
    "`needle.optim.SGD(params, lr=0.01, momentum=0.0, weight_decay=0.0)`\n",
    "\n",
    "Implements stochastic gradient descent (optionally with momentum, shown as $\\beta$ below). \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    u_{t+1} &= \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t) \\\\\n",
    "    \\theta_{t+1} &= \\theta_t - \\alpha u_{t+1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `params` - iterable of parameters of type `needle.nn.Parameter` to optimize\n",
    "- `lr` (*float*) - learning rate\n",
    "- `momentum` (*float*) - momentum factor\n",
    "- `weight_decay` (*float*) - weight decay (L2 penalty)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 84 deselected / 6 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_optim_sgd_vanilla_1 \u001b[32mPASSED\u001b[0m\u001b[32m              [ 16%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_sgd_momentum_1 \u001b[32mPASSED\u001b[0m\u001b[32m             [ 33%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_sgd_weight_decay_1 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_sgd_momentum_weight_decay_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_sgd_layernorm_residual_1 \u001b[32mPASSED\u001b[0m\u001b[32m   [ 83%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_sgd_z_memory_check_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m6 passed\u001b[0m, \u001b[33m84 deselected\u001b[0m\u001b[32m in 0.37s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_optim_sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting optim_sgd...\n",
      "loss 3.2022102\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 3.52s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"optim_sgd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adam\n",
    "`needle.optim.Adam(params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0)`\n",
    "\n",
    "Implements Adam algorithm, proposed in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "u_{t+1} &= \\beta_1 u_t + (1-\\beta_1) \\nabla_\\theta f(\\theta_t) \\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2) (\\nabla_\\theta f(\\theta_t))^2 \\\\\n",
    "\\hat{u}_{t+1} &= u_{t+1} / (1 - \\beta_1^t) \\quad \\text{(bias correction)} \\\\\n",
    "\\hat{v}_{t+1} &= v_{t+1} / (1 - \\beta_2^t) \\quad \\text{(bias correction)}\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\hat{u_{t+1}}/(\\hat{v}_{t+1}^{1/2}+\\epsilon)\n",
    "\\end{split}\n",
    "    \\end{equation}\n",
    "\n",
    "**Important:** Pay attention to whether or not you are applying bias correction.\n",
    "\n",
    "##### Parameters\n",
    "- `params` - iterable of parameters of type `needle.nn.Parameter` to optimize\n",
    "- `lr` (*float*) - learning rate\n",
    "- `beta1` (*float*) - coefficient used for computing running average of gradient\n",
    "- `beta2` (*float*) - coefficient used for computing running average of square of gradient\n",
    "- `eps` (*float*) - term added to the denominator to improve numerical stability\n",
    "- `bias_correction` - whether to use bias correction for $u, v$\n",
    "- `weight_decay` (*float*) - weight decay (L2 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 83 deselected / 7 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_optim_adam_1 \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 14%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_adam_weight_decay_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 28%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_adam_batchnorm_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [ 42%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_adam_batchnorm_eval_mode_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_adam_layernorm_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [ 71%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_adam_weight_decay_bias_correction_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_optim_adam_z_memory_check_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m7 passed\u001b[0m, \u001b[33m83 deselected\u001b[0m\u001b[32m in 0.40s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_optim_adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting optim_adam...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 5.95s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"optim_adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "In this question, you will implement two data primitives: `needle.data.DataLoader` and `needle.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples. \n",
    "\n",
    "For this question, you will be working in `python/needle/data.py`. First, copy your solution to `parse_mnist` from the previous homework into the `parse_mnist` function. \n",
    "\n",
    "### Transformations\n",
    "\n",
    "First we will implement a few transformations that are helpful when working with images. We will stick with a horizontal flip and a random crop for now. Fill out the following functions in `data.py`.\n",
    "___ \n",
    "\n",
    "#### RandomFlipHorizontal\n",
    "`needle.data.RandomFlipHorizontal(p = 0.5)`\n",
    "\n",
    "Flips the image horizontally, with probability `p`.\n",
    "\n",
    "##### Parameters\n",
    "- `p` (*float*) - The probability of flipping the input image.\n",
    "___\n",
    "\n",
    "#### RandomCrop\n",
    "`needle.data.RandomCrop(padding=3)`\n",
    "\n",
    "Padding is added to all side of the image, and then the image is cropped back to it's original size at a random location. Returns an image the same size as the original image.\n",
    "\n",
    "##### Parameters\n",
    "- `padding` (*int*) - The padding on each border of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"flip_horizontal\"\n",
    "!python3 -m pytest -v -k \"random_crop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = np.random.randint(low=-3, high=3+1, size=2)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98688242, 0.30649989, 0.44618113],\n",
       "       [0.2978236 , 0.63813873, 0.44187347],\n",
       "       [0.51371693, 0.92564927, 0.98396174]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.random.random((3, 3))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.98688242, 0.30649989, 0.44618113, 0.        ],\n",
       "       [0.        , 0.2978236 , 0.63813873, 0.44187347, 0.        ],\n",
       "       [0.        , 0.51371693, 0.92564927, 0.98396174, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = 1\n",
    "h, w = img.shape\n",
    "padded_img = np.zeros((h + pad * 2, w + pad * 2))\n",
    "padded_img[pad:-pad, pad:-pad] = img\n",
    "padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.44618113, 0.        ],\n",
       "       [0.44187347, 0.        ],\n",
       "       [0.98396174, 0.        ],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_img[x:, y:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((5, 2))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-2:, :] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2:] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_data.py \n",
      "Submitting flip_horizontal...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 5.61s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_data.py \n",
      "Submitting random_crop...\n",
      "*** img (4, 4, 5)\n",
      "*** shift_x, shift_y 0 0\n",
      "*** padded_img (4, 4, 5)\n",
      "\u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ submit_random_crop ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92msubmit_random_crop\u001b[39;49;00m():\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        tform = ndl.data.RandomCrop(\u001b[94m0\u001b[39;49;00m)\n",
      "        \u001b[94mfor\u001b[39;49;00m _ \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m2\u001b[39;49;00m):\n",
      "            size_a, size_b, size_c = np.random.randint(\u001b[94m4\u001b[39;49;00m,\u001b[94m5\u001b[39;49;00m), np.random.randint(\u001b[94m4\u001b[39;49;00m,\u001b[94m6\u001b[39;49;00m), np.random.randint(\u001b[94m4\u001b[39;49;00m,\u001b[94m7\u001b[39;49;00m)\n",
      ">           mugrade.submit(tform(np.random.rand(size_a, size_b,size_c)))\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_data.py\u001b[0m:141: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.data.RandomCrop object at 0x114a16670>\n",
      "img = array([[[0.84426575, 0.85794562, 0.84725174, 0.6235637 , 0.38438171],\n",
      "        [0.29753461, 0.05671298, 0.27265629, 0.4....05571469, 0.45115921, 0.01998767, 0.44171092],\n",
      "        [0.97958673, 0.35944446, 0.48089353, 0.68866118, 0.88047589]]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__call__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, img):\n",
      "        \u001b[33m\"\"\" Zero pad and then randomly crop an image.\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m         img: H x W x C NDArray of an image\u001b[39;49;00m\n",
      "    \u001b[33m    Return\u001b[39;49;00m\n",
      "    \u001b[33m        H x W x C NAArray of cliped image\u001b[39;49;00m\n",
      "    \u001b[33m    Note: generate the image shifted by shift_x, shift_y specified below\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        shift_x, shift_y = np.random.randint(low=-\u001b[96mself\u001b[39;49;00m.padding, high=\u001b[96mself\u001b[39;49;00m.padding+\u001b[94m1\u001b[39;49;00m, size=\u001b[94m2\u001b[39;49;00m)\n",
      "    \n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** img\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, img.shape)\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** shift_x, shift_y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, shift_x, shift_y)\n",
      "        h, w, c = img.shape\n",
      "        padded_img = np.zeros((h + \u001b[94m2\u001b[39;49;00m * \u001b[96mself\u001b[39;49;00m.padding, w + \u001b[94m2\u001b[39;49;00m * \u001b[96mself\u001b[39;49;00m.padding, c))\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** padded_img\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, padded_img.shape)\n",
      ">       padded_img[\u001b[96mself\u001b[39;49;00m.padding:-\u001b[96mself\u001b[39;49;00m.padding, \u001b[96mself\u001b[39;49;00m.padding:-\u001b[96mself\u001b[39;49;00m.padding, :] = img\n",
      "\u001b[1m\u001b[31mE       ValueError: could not broadcast input array from shape (4,4,5) into shape (0,0,5)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/data.py\u001b[0m:55: ValueError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_data.py::\u001b[1msubmit_random_crop\u001b[0m - ValueError: could not broadcast input array from shape (4,4,5) into shape (...\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[31m in 0.66s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"flip_horizontal\"\n",
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"random_crop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Each `Dataset` subclass must implement three functions: `__init__`, `__len__`, and `__getitem__`. The `__init__` function initializes the images, labels, and transforms. The `__len__` function returns the number of samples in the dataset. The `__getitem__` function retrieves a sample from the dataset at a given index `idx`, calls the transform functions on the image (if applicable), converts the image and label to a numpy array (the data will be converted to Tensors elsewhere). Fill out these functions in the `MNISTDataset` class: \n",
    "___ \n",
    "\n",
    "### MNISTDataset\n",
    "`needle.data.MNISTDataset(image_filesname, label_filesname, transforms)`\n",
    "\n",
    "##### Parameters\n",
    "- `image_filesname` - path of file containing images\n",
    "- `label_filesname` - path of file containing labels\n",
    "- `transforms` - an optional list of transforms to apply to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./python\")\n",
    "import needle as ndl\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "dataset = ndl.data.MNISTDataset(\"data/train-images-idx3-ubyte.gz\", \"data/train-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32),\n",
       " array([0, 4, 1], dtype=uint8))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[np.array([1,2,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAJElEQVR4nGNgGAWjYBSMglEwCkbBKKADYGRkJF4xE+3cQWUAAA0iAAbVbqHcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(dataset[42][0]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.188792 ,  6.261355 ,  8.966858 ,  9.4346485,  9.086626 ,\n",
       "        9.214664 , 10.208544 , 10.649756 ], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_norms = np.array([np.linalg.norm(dataset[idx][0]) for idx in [1,42,1000,2000,3000,4000,5000,5005]])\n",
    "sample_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 89 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_data.py::test_mnist_dataset \u001b[32mPASSED\u001b[0m\u001b[32m                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m89 deselected\u001b[0m\u001b[32m in 0.75s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_mnist_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_data.py \n",
      "Submitting mnist_dataset...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 8.52s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"mnist_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "The Dataloader class provides an interface for assembling mini-batches of examples suitable for training using SGD-based approaches, backed by a Dataset object.  In order to build the typical Dataloader interface (allowing users to iterate over all the mini-batches in the dataset), you will need the implement the `__iter__()` and `__next__()` calls in the class: `__iter__()` is called at the start of iteration, while `__next__()` is called to grab the next mini-batch. Please note that subsequent calls to next will require you to return the following batches, so next is not a pure function.\n",
    "___\n",
    "\n",
    "### Dataloader\n",
    "`needle.data.Dataloader(dataset: Dataset, batch_size: Optional[int] = 1, shuffle: bool = False)`\n",
    "\n",
    "Combines a dataset and a sampler, and provides an iterable over the given dataset. \n",
    "\n",
    "##### Parameters\n",
    "- `dataset` - `needle.data.Dataset` - a dataset \n",
    "- `batch_size` - `int` - what batch size to serve the data in \n",
    "- `shuffle` - `bool` - set to ``True`` to have the data reshuffle at every epoch, default ``False``.\n",
    "___ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(np.arange(20), np.arange(20)))\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5]),\n",
       " array([ 6,  7,  8,  9, 10, 11]),\n",
       " array([12, 13, 14, 15, 16, 17]),\n",
       " array([18, 19])]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 6\n",
    "np.array_split(np.arange(20), range(bs, len(dataset), bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 16,  6, 11,  9,  1,  0, 13,  5,  7, 10, 17, 14,  8, 18, 19, 12,\n",
       "        3, 15,  4])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = np.arange(len(dataset))\n",
    "np.random.shuffle(order)\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2, 16,  6, 11,  9,  1]),\n",
       " array([ 0, 13,  5,  7, 10, 17]),\n",
       " array([14,  8, 18, 19, 12,  3]),\n",
       " array([15,  4])]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split(order, range(bs, len(dataset), bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 88 deselected / 2 selected                                \u001b[0m\n",
      "\n",
      "tests/test_data.py::test_dataloader_mnist \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 50%]\u001b[0m\n",
      "tests/test_data.py::test_dataloader_ndarray \u001b[32mPASSED\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m88 deselected\u001b[0m\u001b[32m in 4.54s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_dataloader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_data.py \n",
      "Submitting dataloader...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 3.56s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"dataloader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Given you have now implemented all the necessary components for our neural network library, let's build and train an MLP ResNet. For this question, you will be working in `apps/mlp_resnet.py`. First, fill out the functions `ResidualBlock` and `MLPResNet` as described below:\n",
    "\n",
    "### ResidualBlock\n",
    "`ResidualBlock(dim, hidden_dim, norm=nn.BatchNorm1d, drop_prob=0.1)`\n",
    "\n",
    "Implements a residual block as follows:\n",
    "\n",
    "![](figures/residualblock.png)\n",
    "\n",
    "where the first linear layer has `in_features=dim` and `out_features=hidden_dim`, and the last linear layer has `out_features=dim`. Returns the block as type `nn.Module`. \n",
    "\n",
    "##### Parameters\n",
    "- `dim` (*int*) - input dim\n",
    "- `hidden_dim` (*int*) - hidden dim\n",
    "- `norm` (*nn.Module*) - normalization method\n",
    "- `drop_prob` (*float*) - dropout probability\n",
    "\n",
    "___\n",
    "\n",
    "### MLPResNet\n",
    "`MLPResNet(dim, hidden_dim=100, num_blocks=3, num_classes=10, norm=nn.BatchNorm1d, drop_prob=0.1)`\n",
    "\n",
    "Implements an MLP ResNet as follows:\n",
    "\n",
    "![](figures/mlp_resnet.png)\n",
    "\n",
    "where the first linear layer has `in_features=dim` and `out_features=hidden_dim`, and each ResidualBlock has `dim=hidden_dim` and `hidden_dim=hidden_dim//2`. Returns a network of type `nn.Module`.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` (*int*) - input dim\n",
    "- `hidden_dim` (*int*) - hidden dim\n",
    "- `num_blocks` (*int*) - number of ResidualBlocks\n",
    "- `num_classes` (*int*) - number of classes\n",
    "- `norm` (*nn.Module*) - normalization method\n",
    "- `drop_prob` (*float*) - dropout probability (0.1)\n",
    "___ \n",
    "\n",
    "Once you have the deep learning model architecture correct, let's train the network using our new neural network library components. Specifically, implement the functions `epoch` and `train_mnist`.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "`epoch(dataloader, model, opt=None)`\n",
    "\n",
    "Executes one epoch of training or evaluation, iterating over the entire training dataset once (just like `nn_epoch` from previous homeworks). Returns the average error rate **(changed from accuracy)** (as a *float*) and the average loss over all samples (as a *float*). Set the model to `training` mode at the beginning of the function if `opt` is given; set the model to `eval` if `opt` is not given (i.e. `None`).\n",
    "\n",
    "##### Parameters\n",
    "- `dataloader` (*`needle.data.DataLoader`*) - dataloader returning samples from the training dataset\n",
    "- `model` (*`needle.nn.Module`*) - neural network\n",
    "- `opt` (*`needle.optim.Optimizer`*) - optimizer instance, or `None`\n",
    "\n",
    "___\n",
    "\n",
    "### Train Mnist\n",
    "\n",
    "`train_mnist(batch_size=100, epochs=10, optimizer=ndl.optim.Adam, lr=0.001, weight_decay=0.001, hidden_dim=100, data_dir=\"data\")`\n",
    "                \n",
    "Initializes a training dataloader (with `shuffle` set to `True`) and a test dataloader for MNIST data, and trains an `MLPResNet` using the given optimizer (if `opt` is not None) and the softmax loss for a given number of epochs. Returns a tuple of the training accuracy, training loss, test accuracy, test loss computed in the last epoch of training. If any parameters are not specified, use the default parameters.\n",
    "\n",
    "##### Parameters\n",
    "- `batch_size` (*int*) - batch size to use for train and test dataloader\n",
    "- `epochs` (*int*) - number of epochs to train for\n",
    "- `optimizer` (*`needle.optim.Optimizer` type*) - optimizer type to use\n",
    "- `lr` (*float*) - learning rate \n",
    "- `weight_decay` (*float*) - weight decay\n",
    "- `hidden_dim` (*int*) - hidden dim for `MLPResNet`\n",
    "- `data_dir` (*int*) - directory containing MNIST image/label files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0 -- /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/cmu_env/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 90 items / 80 deselected / 10 selected                               \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py::test_mlp_residual_block_num_params_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_residual_block_num_params_2 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_residual_block_forward_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_resnet_num_params_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_resnet_num_params_2 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_resnet_forward_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_resnet_forward_2 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_train_epoch_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_eval_epoch_1 \u001b[31mFAILED\u001b[0m\n",
      "tests/test_nn_and_optim.py::test_mlp_train_mnist_1 \u001b[31mFAILED\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________ test_mlp_residual_block_num_params_1 _____________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_residual_block_num_params_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(residual_block_num_params(\u001b[94m15\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, nn.BatchNorm1d),\n",
      "            np.array(\u001b[94m111\u001b[39;49;00m), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max absolute difference: 26\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max relative difference: 0.23423423\u001b[0m\n",
      "\u001b[1m\u001b[31mE        x: array(85)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        y: array(111)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1035: AssertionError\n",
      "\u001b[31m\u001b[1m_____________________ test_mlp_residual_block_num_params_2 _____________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_residual_block_num_params_2\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(residual_block_num_params(\u001b[94m784\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, nn.LayerNorm1d),\n",
      "            np.array(\u001b[94m159452\u001b[39;49;00m), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max absolute difference: 1368\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max relative difference: 0.00857938\u001b[0m\n",
      "\u001b[1m\u001b[31mE        x: array(158084)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        y: array(159452)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1039: AssertionError\n",
      "\u001b[31m\u001b[1m______________________ test_mlp_residual_block_forward_1 _______________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_residual_block_forward_1\u001b[39;49;00m():\n",
      "        np.testing.assert_allclose(\n",
      ">           residual_block_forward(\u001b[94m15\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, nn.LayerNorm1d, \u001b[94m0.5\u001b[39;49;00m),\n",
      "            np.array([[\n",
      "                \u001b[94m0.\u001b[39;49;00m, \u001b[94m1.358399\u001b[39;49;00m, \u001b[94m0.\u001b[39;49;00m, \u001b[94m1.384224\u001b[39;49;00m, \u001b[94m0.\u001b[39;49;00m, \u001b[94m0.\u001b[39;49;00m, \u001b[94m0.255451\u001b[39;49;00m, \u001b[94m0.077662\u001b[39;49;00m, \u001b[94m0.\u001b[39;49;00m,\n",
      "                \u001b[94m0.939582\u001b[39;49;00m, \u001b[94m0.525591\u001b[39;49;00m, \u001b[94m1.99213\u001b[39;49;00m, \u001b[94m0.\u001b[39;49;00m, \u001b[94m0.\u001b[39;49;00m, \u001b[94m1.012827\u001b[39;49;00m\n",
      "            ]],\n",
      "            dtype=np.float32),\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m,\n",
      "        )\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1044: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:319: in residual_block_forward\n",
      "    output_tensor = ResidualBlock(dim, hidden_dim, norm, drop_prob)(input_tensor)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:255: in forward\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.fn(x) + x\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:220: in forward\n",
      "    shaped_weight = \u001b[96mself\u001b[39;49;00m.weight.reshape((\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim)).broadcast_to((m, n))\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:396: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.BroadcastTo(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:241: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in broadcast_to\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)\n",
      "shape = (1, 15), subok = False, readonly = True\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        extras = []\n",
      ">       it = np.nditer(\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,10)  and requested shape (1,15)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "\u001b[31m\u001b[1m_________________________ test_mlp_resnet_num_params_1 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_resnet_num_params_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(mlp_resnet_num_params(\u001b[94m150\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m10\u001b[39;49;00m, nn.LayerNorm1d),\n",
      "            np.array(\u001b[94m68360\u001b[39;49;00m), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max absolute difference: 1000\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max relative difference: 0.01462844\u001b[0m\n",
      "\u001b[1m\u001b[31mE        x: array(67360)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        y: array(68360)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1055: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_mlp_resnet_num_params_2 _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_resnet_num_params_2\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(mlp_resnet_num_params(\u001b[94m10\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m, nn.BatchNorm1d),\n",
      "            np.array(\u001b[94m21650\u001b[39;49;00m), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
      "\u001b[1m\u001b[31mE       \u001b[0m\n",
      "\u001b[1m\u001b[31mE       Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max absolute difference: 5100\u001b[0m\n",
      "\u001b[1m\u001b[31mE       Max relative difference: 0.23556582\u001b[0m\n",
      "\u001b[1m\u001b[31mE        x: array(16550)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        y: array(21650)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1059: AssertionError\n",
      "\u001b[31m\u001b[1m__________________________ test_mlp_resnet_forward_1 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_resnet_forward_1\u001b[39;49;00m():\n",
      "        np.testing.assert_allclose(\n",
      ">           mlp_resnet_forward(\u001b[94m10\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, nn.LayerNorm1d, \u001b[94m0.5\u001b[39;49;00m),\n",
      "            np.array([[\u001b[94m3.046162\u001b[39;49;00m, \u001b[94m1.44972\u001b[39;49;00m, -\u001b[94m1.921363\u001b[39;49;00m, \u001b[94m0.021816\u001b[39;49;00m, -\u001b[94m0.433953\u001b[39;49;00m],\n",
      "                      [\u001b[94m3.489114\u001b[39;49;00m, \u001b[94m1.820994\u001b[39;49;00m, -\u001b[94m2.111306\u001b[39;49;00m, \u001b[94m0.226388\u001b[39;49;00m, -\u001b[94m1.029428\u001b[39;49;00m]],\n",
      "                     dtype=np.float32),\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1064: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:329: in mlp_resnet_forward\n",
      "    output_tensor = MLPResNet(dim, hidden_dim, num_blocks, num_classes, norm, drop_prob)(input_tensor)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:255: in forward\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.fn(x) + x\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:179: in forward\n",
      "    shaped_weight = \u001b[96mself\u001b[39;49;00m.weight.reshape((\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim)).broadcast_to((m, n))\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:396: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.BroadcastTo(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:241: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in broadcast_to\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[1., 1.]], dtype=float32), shape = (2, 5), subok = False\n",
      "readonly = True\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        extras = []\n",
      ">       it = np.nditer(\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,2)  and requested shape (2,5)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "\u001b[31m\u001b[1m__________________________ test_mlp_resnet_forward_2 ___________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_resnet_forward_2\u001b[39;49;00m():\n",
      "        np.testing.assert_allclose(\n",
      ">           mlp_resnet_forward(\u001b[94m15\u001b[39;49;00m, \u001b[94m25\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m14\u001b[39;49;00m, nn.BatchNorm1d, \u001b[94m0.0\u001b[39;49;00m),\n",
      "            np.array([[\n",
      "                \u001b[94m0.92448235\u001b[39;49;00m, -\u001b[94m2.745743\u001b[39;49;00m, -\u001b[94m1.5077105\u001b[39;49;00m, \u001b[94m1.130784\u001b[39;49;00m, -\u001b[94m1.2078242\u001b[39;49;00m,\n",
      "                -\u001b[94m0.09833566\u001b[39;49;00m, -\u001b[94m0.69301605\u001b[39;49;00m, \u001b[94m2.8945382\u001b[39;49;00m, \u001b[94m1.259397\u001b[39;49;00m, \u001b[94m0.13866742\u001b[39;49;00m,\n",
      "                -\u001b[94m2.963875\u001b[39;49;00m, -\u001b[94m4.8566914\u001b[39;49;00m, \u001b[94m1.7062538\u001b[39;49;00m, -\u001b[94m4.846424\u001b[39;49;00m\n",
      "            ],\n",
      "            [\n",
      "                \u001b[94m0.6653336\u001b[39;49;00m, -\u001b[94m2.4708004\u001b[39;49;00m, \u001b[94m2.0572243\u001b[39;49;00m, -\u001b[94m1.0791507\u001b[39;49;00m, \u001b[94m4.3489094\u001b[39;49;00m,\n",
      "                \u001b[94m3.1086435\u001b[39;49;00m, \u001b[94m0.0304327\u001b[39;49;00m, -\u001b[94m1.9227124\u001b[39;49;00m, -\u001b[94m1.416201\u001b[39;49;00m, -\u001b[94m7.2151937\u001b[39;49;00m,\n",
      "                -\u001b[94m1.4858506\u001b[39;49;00m, \u001b[94m7.1039696\u001b[39;49;00m, -\u001b[94m2.1589825\u001b[39;49;00m, -\u001b[94m0.7593413\u001b[39;49;00m\n",
      "            ]],\n",
      "            dtype=np.float32),\n",
      "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\n",
      "            atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1073: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:329: in mlp_resnet_forward\n",
      "    output_tensor = MLPResNet(dim, hidden_dim, num_blocks, num_classes, norm, drop_prob)(input_tensor)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:255: in forward\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.fn(x) + x\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:179: in forward\n",
      "    shaped_weight = \u001b[96mself\u001b[39;49;00m.weight.reshape((\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim)).broadcast_to((m, n))\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:396: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.BroadcastTo(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:241: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in broadcast_to\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)\n",
      "shape = (2, 25), subok = False, readonly = True\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        extras = []\n",
      ">       it = np.nditer(\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,12)  and requested shape (2,25)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "\u001b[31m\u001b[1m____________________________ test_mlp_train_epoch_1 ____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_train_epoch_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(train_epoch_1(\u001b[94m5\u001b[39;49;00m, \u001b[94m250\u001b[39;49;00m, ndl.optim.Adam, lr=\u001b[94m0.01\u001b[39;49;00m, weight_decay=\u001b[94m0.1\u001b[39;49;00m),\n",
      "            np.array([\u001b[94m0.675267\u001b[39;49;00m, \u001b[94m1.84043\u001b[39;49;00m]), rtol=\u001b[94m0.0001\u001b[39;49;00m, atol=\u001b[94m0.0001\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1089: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:344: in train_epoch_1\n",
      "    \u001b[94mreturn\u001b[39;49;00m np.array(epoch(train_dataloader, model, opt))\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "dataloader = <needle.data.DataLoader object at 0x104aca7f0>\n",
      "model = <needle.nn.Sequential object at 0x104bdaeb0>\n",
      "opt = <needle.optim.Adam object at 0x104aca9d0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mepoch\u001b[39;49;00m(dataloader, model, opt=\u001b[94mNone\u001b[39;49;00m):\n",
      "        np.random.seed(\u001b[94m4\u001b[39;49;00m)\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m opt \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            model.eval()\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      ">           model.training()\n",
      "\u001b[1m\u001b[31mE           TypeError: 'bool' object is not callable\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mapps/mlp_resnet.py\u001b[0m:48: TypeError\n",
      "\u001b[31m\u001b[1m____________________________ test_mlp_eval_epoch_1 _____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_eval_epoch_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(eval_epoch_1(\u001b[94m10\u001b[39;49;00m, \u001b[94m150\u001b[39;49;00m),\n",
      "            np.array([\u001b[94m0.9164\u001b[39;49;00m , \u001b[94m4.137814\u001b[39;49;00m]), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1093: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:358: in eval_epoch_1\n",
      "    \u001b[94mreturn\u001b[39;49;00m np.array(epoch(test_dataloader, model))\n",
      "\u001b[1m\u001b[31mapps/mlp_resnet.py\u001b[0m:53: in epoch\n",
      "    logits = model(batch)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:255: in forward\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.fn(x) + x\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:139: in forward\n",
      "    x = module(x)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:75: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.forward(*args, **kwargs)\n",
      "\u001b[1m\u001b[31mpython/needle/nn.py\u001b[0m:179: in forward\n",
      "    shaped_weight = \u001b[96mself\u001b[39;49;00m.weight.reshape((\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim)).broadcast_to((m, n))\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:396: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m needle.ops.BroadcastTo(shape)(\u001b[96mself\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:126: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:289: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:153: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:241: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.broadcast_to(a, \u001b[96mself\u001b[39;49;00m.shape)\n",
      "\u001b[1m\u001b[31m<__array_function__ internals>\u001b[0m:180: in broadcast_to\n",
      "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:413: in broadcast_to\n",
      "    \u001b[94mreturn\u001b[39;49;00m _broadcast_to(array, shape, subok=subok, readonly=\u001b[94mTrue\u001b[39;49;00m)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "array = array([[1., 1., 1., 1., 1.]], dtype=float32), shape = (150, 10)\n",
      "subok = False, readonly = True\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m_broadcast_to\u001b[39;49;00m(array, shape, subok, readonly):\n",
      "        shape = \u001b[96mtuple\u001b[39;49;00m(shape) \u001b[94mif\u001b[39;49;00m np.iterable(shape) \u001b[94melse\u001b[39;49;00m (shape,)\n",
      "        array = np.array(array, copy=\u001b[94mFalse\u001b[39;49;00m, subok=subok)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m shape \u001b[95mand\u001b[39;49;00m array.shape:\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcannot broadcast a non-scalar to a scalar array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(size < \u001b[94m0\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m size \u001b[95min\u001b[39;49;00m shape):\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mall elements of broadcast shape must be non-\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mnegative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        extras = []\n",
      ">       it = np.nditer(\n",
      "            (array,), flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mmulti_index\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrefs_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mzerosize_ok\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + extras,\n",
      "            op_flags=[\u001b[33m'\u001b[39;49;00m\u001b[33mreadonly\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itershape=shape, order=\u001b[33m'\u001b[39;49;00m\u001b[33mC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (1,5)  and requested shape (150,10)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../cmu_env/lib/python3.9/site-packages/numpy/lib/stride_tricks.py\u001b[0m:349: ValueError\n",
      "\u001b[31m\u001b[1m____________________________ test_mlp_train_mnist_1 ____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_mlp_train_mnist_1\u001b[39;49;00m():\n",
      ">       np.testing.assert_allclose(train_mnist_1(\u001b[94m250\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, ndl.optim.SGD, \u001b[94m0.001\u001b[39;49;00m, \u001b[94m0.01\u001b[39;49;00m, \u001b[94m100\u001b[39;49;00m),\n",
      "            np.array([\u001b[94m0.4875\u001b[39;49;00m , \u001b[94m1.462595\u001b[39;49;00m, \u001b[94m0.3245\u001b[39;49;00m , \u001b[94m1.049429\u001b[39;49;00m]), rtol=\u001b[94m0.001\u001b[39;49;00m, atol=\u001b[94m0.001\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:1097: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nn_and_optim.py\u001b[0m:362: in train_mnist_1\n",
      "    out = train_mnist(batch_size, epochs, optimizer, lr, weight_decay, hidden_dim, data_dir=\u001b[33m\"\u001b[39;49;00m\u001b[33m./data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mapps/mlp_resnet.py\u001b[0m:91: in train_mnist\n",
      "    train_loss = epoch(mnist_train_dataloader, model, opt)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "dataloader = <needle.data.DataLoader object at 0x104c16820>\n",
      "model = <needle.nn.Sequential object at 0x104ae80d0>\n",
      "opt = <needle.optim.SGD object at 0x104c169a0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mepoch\u001b[39;49;00m(dataloader, model, opt=\u001b[94mNone\u001b[39;49;00m):\n",
      "        np.random.seed(\u001b[94m4\u001b[39;49;00m)\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m opt \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            model.eval()\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      ">           model.training()\n",
      "\u001b[1m\u001b[31mE           TypeError: 'bool' object is not callable\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mapps/mlp_resnet.py\u001b[0m:48: TypeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_residual_block_num_params_1\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_residual_block_num_params_2\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_residual_block_forward_1\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_resnet_num_params_1\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_resnet_num_params_2\u001b[0m - AssertionError: \n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_resnet_forward_1\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_resnet_forward_2\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_train_epoch_1\u001b[0m - TypeError: 'bool' object is not callable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_eval_epoch_1\u001b[0m - ValueError: operands could not be broadcast together with remapped shapes [...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nn_and_optim.py::\u001b[1mtest_mlp_train_mnist_1\u001b[0m - TypeError: 'bool' object is not callable\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m10 failed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 1.07s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"test_mlp\" -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /Users/aleksandrfida/Desktop/dl_courses/cmu_dl_systems/dl_systems_hw2\n",
      "plugins: anyio-3.6.2\n",
      "collected 18 items / 17 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_nn_and_optim.py \n",
      "Submitting mlp_resnet...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m17 deselected\u001b[0m\u001b[32m in 13.79s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m mugrade submit _cKAK1g4ruW8KYCMcW7yc -k \"mlp_resnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage to experiment with the `mlp_resnet.py` training script.\n",
    "You can investigate the effect of using different initializers on the Linear layers,\n",
    "increasing the dropout probability,\n",
    "or adding transforms (via a list to the `transforms=` keyword argument of Dataset)\n",
    "such as random cropping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cmu_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "998dd1799808899da159c73324c38e2b8ac5d39cb24bcbc78a11f59c32aa7122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
